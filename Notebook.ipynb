{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to network analytics with Python\n",
    "### With applications in actuarial science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Network Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with generating a random Barab√°si-Albert network to show how to construct such random networks. Afterwards, we analyse its properties and calculate some network features. This allows us to illustrate the meaning of the *scale-free* property. \n",
    "\n",
    "In this part of the example, we will rely on the `igraph` library to generate the random network and calculate the metrics. The network itself will consist of 2000 nodes. The generator adds them one at a time. Additionally, we specify that each new node should have an edge to one other nodes. This end point for the new edge will be chosen randomly from the nodes already present, taking into account the degree of the other nodes (preferential attachment). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import igraph as ig # to handle graph data and provide calculations and visualizations\n",
    "import numpy as np # to handle numerical data\n",
    "import pandas as pd # to handle tabular data\n",
    "import matplotlib.pyplot as plt # to provide visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph\n",
    "graph = ig.Graph()\n",
    "\n",
    "# Create the Barabasi-Albert network\n",
    "# The graph should contain 200 nodes, with each new node connecting to one other\n",
    "ba_graph = graph.Barabasi(200, m=1) \n",
    "\n",
    "print(\"Number of nodes: \", ba_graph.vcount())\n",
    "print(\"Number of edges: \", ba_graph.ecount())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `igraph` package allows us to quickly and easily calculate a whole range of metrics for this network. In this example, we will stick to four of them, in line with the ones discussed in the slide:\n",
    "* **Degree**: number of connections for each node; \n",
    "* **Betweenness centrality**: relative number of shortest paths passing through this node;\n",
    "* **Closeness centrality**: the inverse of the average distance to the other nodes;\n",
    "* **PageRank**: importance of the node based on number of connections and importance neighbours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree = ba_graph.degree() #Degree\n",
    "betw = ba_graph.betweenness() #Betweenness\n",
    "cls = ba_graph.closeness() #Closeness\n",
    "pgrnk = ba_graph.pagerank() #PageRank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with plotting the distributions of the four metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,2, figsize=(15,10))\n",
    "ax[0,0].hist(degree, bins=10)\n",
    "ax[0,0].set_title('Degree Distribution')\n",
    "ax[0,1].hist(betw, bins=10)\n",
    "ax[0,1].set_title('Betweenness')\n",
    "ax[1,0].hist(cls, bins=10)\n",
    "ax[1,0].set_title('Closeness')\n",
    "ax[1,1].hist(pgrnk, bins=10)\n",
    "ax[1,1].set_title('PageRank')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we plot the degree distribution. The distribution of a Barabasi-Albert network exhibits the scale-free property. A visual test for this consists of plotting the density against the degree with both axes on a log-scale. The scale-free property is likely present when the plot resembles a straight line. \n",
    "\n",
    "This result is often present in real-world networks. It means that many nodes have only very few connection, while there are a limit number of nodes with a very high number of connections (called hubs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pk(degree, values):\n",
    "    num_nodes = len(values)\n",
    "    count_degree = values.count(degree)\n",
    "    density = count_degree/num_nodes\n",
    "    return density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_degrees = range(min(degree), max(degree)+1)\n",
    "list_densities = []\n",
    "for i in list_degrees:\n",
    "    density = Pk(i, degree)\n",
    "    list_densities.append(density)\n",
    "\n",
    "plt.scatter(list_degrees, list_densities)\n",
    "plt.xlabel('Degree (log scale)')\n",
    "plt.ylabel('Density (log scale)')\n",
    "plt.title('Degree Distribution')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A general characteristic of networks, is that there is a strong correlation between the degree and the betweenness centrality. This will be tested and shown in the next piece of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(degree, betw)\n",
    "plt.xlabel('Degree')\n",
    "plt.ylabel('Betweenness')\n",
    "plt.title('Betweenness vs Degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(degree, betw)\n",
    "plt.xlabel('Degree (log scale)')\n",
    "plt.ylabel('Betweenness (log scale)')\n",
    "plt.title('Betweenness vs Degree')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part, we show how to visualise the network. Additionally, we will change the size of the node depending on the betweenness centrality. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the layout\n",
    "layout_fr = ba_graph.layout(\"fr\")\n",
    "\n",
    "#Define style from network plotting\n",
    "visual_style = {}\n",
    "visual_style[\"vertex_size\"] = 5\n",
    "visual_style[\"vertex_color\"] = \"red\"\n",
    "visual_style[\"layout\"] = layout_fr\n",
    "visual_style[\"bbox\"] = (600, 600)\n",
    "visual_style[\"margin\"] = 20\n",
    "\n",
    "ig.plot(ba_graph, **visual_style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = ba_graph.vcount()\n",
    "ba_graph.vs['vertex_size'] = [5]\n",
    "for i in range(num_nodes):\n",
    "    betw_node = betw[i] + 1 # Avoid 0\n",
    "    ba_graph.vs[i]['vertex_size'] = np.log(betw_node)*1.5 +1 # Avoid that 1 maps to 0 after log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the layout\n",
    "layout_fr = ba_graph.layout(\"fr\")\n",
    "\n",
    "#Define style from network plotting\n",
    "visual_style = {}\n",
    "visual_style[\"vertex_size\"] = ba_graph.vs['vertex_size']\n",
    "visual_style[\"vertex_color\"] = \"red\"\n",
    "visual_style[\"layout\"] = layout_fr\n",
    "visual_style[\"bbox\"] = (600, 600)\n",
    "visual_style[\"margin\"] = 20\n",
    "\n",
    "ig.plot(ba_graph, **visual_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen in the slides, in most networks, only a fraction of the possible edges are actually present in the network. This results in a very sparse adjacency matrix (containing mostly 0). To illustrate this, we are going to visualise the adjacency matrix of the random network below. A black pixel signifies that there is an edge, while white space means that there is no connection between the two nodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacency_matrix = ba_graph.get_adjacency()\n",
    "plt.imshow(adjacency_matrix, cmap='binary', interpolation='none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Health Care Provider Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to move to the data set that will also be used later in this notebook when constructing a classification model. The data itself is taken from Kaggle, and consists of health care insurance claims, with information of the claims and patients involved. \n",
    "The claim data is split into two data sets (which we will combine for ease of use): in-patient data and out-patient data. These contain patient data for patient admitted in the hospital and those who aren't, respectively. \n",
    "\n",
    "The set-up of the data is to uncover those health care providers that are indulging in Medicare fraud.\n",
    "\n",
    "In the following pieces of code, we are going to do a first analysis of the data. We will construct a network containing the following four types of nodes (heterogeneous network):\n",
    "* Providers\n",
    "* Physicians\n",
    "* Patients\n",
    "* Claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_inpatients = pd.read_csv('data/Train_Inpatientdata-1542865627584.csv')\n",
    "claims_outpatients = pd.read_csv('data/Train_Outpatientdata-1542865627584.csv')\n",
    "patient_data = pd.read_csv('data/Train_Beneficiarydata-1542865627584.csv')\n",
    "labels = pd.read_csv('data/Train-1542865627584.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_inpatients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_outpatients.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a first pre-processing step, we are going to convert the fraud labels into binary labels, i.e., 0 and 1. Here, 1 will indicate that the provider is potentially fraudulent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_label = {'No': 0, 'Yes': 1}\n",
    "labels['PotentialFraud'] = labels['PotentialFraud'].map(map_label)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is now easy to do a first analysis of the prevalence of the fraud labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prevalence = labels['PotentialFraud'].mean()\n",
    "print(\"Prevalence of fraud: \", round(prevalence*100, 2), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network itself will be purely based on the claim information. So we set up a claim as central node, and link it to the provider, patient an attending physician. Note that it is possible to also add the other physicians involved, but we do not do it here for simplicity. \n",
    "\n",
    "For this example, we will work with a subset of claims. We will take 100 claims for the inpatients and 100 for the outpatients, to have some clear visualisations. The concepts and code stay the same larger networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "# Create a new graph\n",
    "healthcare_graph = ig.Graph()\n",
    "\n",
    "# Take first 1000 rows for inpatients and outpatients\n",
    "claims_inpatients = claims_inpatients.iloc[:100]\n",
    "claims_outpatients = claims_outpatients.iloc[:100]\n",
    "\n",
    "# Add nodes for providers, physicians, patients, and claims\n",
    "providers = list(np.unique(claims_inpatients['Provider'].tolist() + claims_outpatients['Provider'].tolist()))\n",
    "physicians = list(np.unique(claims_inpatients['AttendingPhysician'].tolist() + claims_outpatients['AttendingPhysician'].tolist()))\n",
    "patients = list(np.unique(claims_inpatients['BeneID'].tolist() + claims_outpatients['BeneID'].tolist()))\n",
    "claims = list(np.unique(claims_inpatients['ClaimID'].tolist() + claims_outpatients['ClaimID'].tolist()))\n",
    "\n",
    "# Add vertices to the graph\n",
    "healthcare_graph.add_vertices(providers + physicians + patients + claims)\n",
    "\n",
    "# Add edges based on the relationships in the data\n",
    "edges = []\n",
    "\n",
    "# Add edges for inpatients\n",
    "for _, row in claims_inpatients.iterrows():\n",
    "    edges.append((row['Provider'], row['ClaimID']))\n",
    "    edges.append((row['AttendingPhysician'], row['ClaimID']))\n",
    "    edges.append((row['BeneID'], row['ClaimID']))\n",
    "\n",
    "# Add edges for outpatients\n",
    "for _, row in claims_outpatients.iterrows():\n",
    "    edges.append((row['Provider'], row['ClaimID']))\n",
    "    edges.append((row['AttendingPhysician'], row['ClaimID']))\n",
    "    edges.append((row['BeneID'], row['ClaimID']))\n",
    "\n",
    "# Add edges to the graph\n",
    "edges_filtered = []\n",
    "for edge in tqdm(edges):\n",
    "    # Test if none of the two elements is NaN\n",
    "    if edge[0] == edge[0] and edge[1] == edge[1]:\n",
    "        edges_filtered.append(edge)\n",
    "\n",
    "healthcare_graph.add_edges(edges_filtered)\n",
    "\n",
    "# Print summary of the graph\n",
    "print(healthcare_graph.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colours depending on type of node\n",
    "num_nodes = healthcare_graph.vcount()\n",
    "healthcare_graph.vs['vertex_color'] = [\"red\"]*num_nodes\n",
    "\n",
    "for i in tqdm(range(num_nodes)):\n",
    "    name = healthcare_graph.vs[i]['name']\n",
    "    if name in providers:\n",
    "        healthcare_graph.vs[i]['vertex_color'] = \"blue\"\n",
    "    elif name in physicians:\n",
    "        healthcare_graph.vs[i]['vertex_color'] = \"green\"\n",
    "    elif name in patients:\n",
    "        healthcare_graph.vs[i]['vertex_color'] = \"yellow\"\n",
    "    elif name in claims:\n",
    "        healthcare_graph.vs[i]['vertex_color'] = \"purple\"\n",
    "\n",
    "# Calculate the layout\n",
    "layout_fr = healthcare_graph.layout(\"fr\")\n",
    "\n",
    "#Define style from network plotting\n",
    "visual_style = {}\n",
    "visual_style[\"vertex_size\"] = 4\n",
    "visual_style[\"vertex_color\"] = healthcare_graph.vs['vertex_color']\n",
    "visual_style[\"layout\"] = layout_fr\n",
    "visual_style[\"bbox\"] = (600, 600)\n",
    "visual_style[\"margin\"] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ig.plot(healthcare_graph, **visual_style)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Representation Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part of the notebook, we look at constructing different network embeddings. Two will be handles here, namely `node2vec` and `graphSAGE`. \n",
    "\n",
    "To apply both of these methods, we need a homogeneous network. As the labels are at health care provider level, we will link providers that share the same beneficiaries or physicians.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data to have all points \n",
    "claims_inpatients = pd.read_csv('data/Train_Inpatientdata-1542865627584.csv')\n",
    "claims_outpatients = pd.read_csv('data/Train_Outpatientdata-1542865627584.csv')\n",
    "\n",
    "# Extract the providers and beneficiaries from the data\n",
    "inpatients_providers = claims_inpatients[['Provider', 'BeneID']]\n",
    "outpatients_providers = claims_outpatients[['Provider', 'BeneID']]\n",
    "\n",
    "# Merge the two dataframes\n",
    "providers_patients = pd.concat([inpatients_providers, outpatients_providers])\n",
    "\n",
    "# Join the tables to have providers that share patients\n",
    "providers_shared_patients = providers_patients.merge(providers_patients, on='BeneID')\n",
    "providers_shared_patients = providers_shared_patients[providers_shared_patients['Provider_x'] != providers_shared_patients['Provider_y']]\n",
    "providers_shared_patients = providers_shared_patients[['Provider_x', 'Provider_y']]\n",
    "providers_shared_patients = providers_shared_patients.drop_duplicates()\n",
    "providers_shared_patients.columns = ['txId1', 'txId2']\n",
    "\n",
    "# Extract the providers and physicians from the data\n",
    "inpatients_providers = claims_inpatients[['Provider', 'AttendingPhysician']]\n",
    "outpatients_providers = claims_outpatients[['Provider', 'AttendingPhysician']]\n",
    "\n",
    "# Merge the two dataframes\n",
    "providers_physicians = pd.concat([inpatients_providers, outpatients_providers])\n",
    "\n",
    "# Join the tables to have providers that share physicians\n",
    "providers_shared_physicians = providers_physicians.merge(providers_physicians, on='AttendingPhysician')\n",
    "providers_shared_physicians = providers_shared_physicians[providers_shared_physicians['Provider_x'] != providers_shared_physicians['Provider_y']]\n",
    "providers_shared_physicians = providers_shared_physicians[['Provider_x', 'Provider_y']]\n",
    "providers_shared_physicians = providers_shared_physicians.drop_duplicates()\n",
    "providers_shared_physicians.columns = ['txId1', 'txId2']\n",
    "\n",
    "# Define one dataframe with all the edges\n",
    "edges = pd.concat([providers_shared_patients, providers_shared_physicians])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a dedicated implementation for `node2vec`, which requires to define the network in `networkx`. An implementation is also available in `pytorch-geometric` but this often gives problems when installing, making it harder to use right away. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "import networkx as nx\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "EMBEDDING_FILENAME='embedding.csv'\n",
    "EMBEDDING_MODEL_FILENAME='n2v_embedding.model'\n",
    "\n",
    "p = 1\n",
    "q = 0.5\n",
    "dimensions = 16\n",
    "num_walks = 1\n",
    "walk_length = 5\n",
    "window_size = 2\n",
    "num_iter = 1\n",
    "workers = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of workers: ', workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2vec_embedding(graph, name):\n",
    "    node2vec = Node2Vec(graph, dimensions=dimensions, walk_length=walk_length, num_walks=num_walks, workers=workers, p=p, q=q)\n",
    "    # Embed nodes\n",
    "    model = node2vec.fit(window=window_size, min_count=1, batch_words=4)  # Any keywords acceptable by gensim.Word2Vec can be passed, `dimensions` and `workers` are automatically passed (from the Node2Vec constructor)\n",
    "\n",
    "\n",
    "    # Save embeddings for later use\n",
    "    model.wv.save_word2vec_format(EMBEDDING_FILENAME)\n",
    "\n",
    "    # Save model for later use\n",
    "    model.save(EMBEDDING_MODEL_FILENAME)\n",
    "\n",
    "    def get_embedding(u):\n",
    "        return model.wv[u]\n",
    "\n",
    "    return get_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = nx.Graph()\n",
    "edges = edges.iloc[:50000] # Take only the first 50,000 edges\n",
    "graph.add_edges_from(zip(edges['txId1'], edges['txId2']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes and edges\n",
    "num_nodes = graph.number_of_nodes()\n",
    "num_edges = graph.number_of_edges()\n",
    "\n",
    "\n",
    "# Average degree\n",
    "avg_degree = sum(dict(graph.degree()).values()) / num_nodes\n",
    "\n",
    "# Clustering coefficient\n",
    "clustering_coefficient = nx.average_clustering(graph)\n",
    "\n",
    "# Print the results\n",
    "print(\"Number of nodes:\", num_nodes)\n",
    "print(\"Number of edges:\", num_edges)\n",
    "print(\"Average degree:\", avg_degree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_train = node2vec_embedding(graph, \"Healthcare\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coordinates of the embedding of one provider\n",
    "embedding_train('PRV55912')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "data_dict['txId'] = []\n",
    "for i in range(dimensions):\n",
    "    data_dict['embedding_'+str(i)] = []\n",
    "\n",
    "data_dict['y'] = []\n",
    "\n",
    "for node in tqdm(list(graph.nodes)):\n",
    "    data_dict['txId'].append(node)\n",
    "\n",
    "    embedding = embedding_train(node)\n",
    "    for i in range(dimensions):\n",
    "        data_dict['embedding_'+str(i)].append(embedding[i])\n",
    "\n",
    "    label = labels[labels['Provider'] == node]['PotentialFraud'].values\n",
    "    data_dict['y'].append(int(label))\n",
    "\n",
    "embedding_df = pd.DataFrame(data_dict)\n",
    "embedding_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = embedding_df.drop(columns=['txId', 'y'])\n",
    "y = embedding_df['y']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.4, random_state=1997)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=1997)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict_proba(X_test)[:,1] # Probability of being fraud\n",
    "auc_roc = roc_auc_score(y_test, y_pred)\n",
    "auc_pr = average_precision_score(y_test, y_pred)\n",
    "\n",
    "print(\"Label proportion in test set: \", y_test.mean())\n",
    "print(\"AUC-ROC: \", auc_roc)\n",
    "print(\"AUC-PR: \", auc_pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now move to `pytorch-geometric` for graphSAGE. There are two main things to remember when trying to use `pytorch-geometric` for network analytics:\n",
    "1) All nodes should be provided using an integer index starting from 0;\n",
    "2) When defining the edge-list, `pytorch-geometric` always assumes that you are providing a directed network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all nodes and define a mapping to be applied\n",
    "nodes = list(set(edges['txId1'].tolist() + edges['txId2'].tolist())) # Defining is as a set removes duplicates\n",
    "nodes = pd.DataFrame(nodes, columns=['txId'])\n",
    "labels = labels[labels['Provider'].isin(nodes['txId'])]\n",
    "map_id = {j:i for i,j in enumerate(nodes['txId'])}\n",
    "\n",
    "# Apply the mapping to the nodes\n",
    "nodes['txId'] = nodes['txId'].map(map_id)\n",
    "\n",
    "# Apply the mapping to the edges\n",
    "edges['txId1'] = edges['txId1'].map(map_id)\n",
    "edges['txId2'] = edges['txId2'].map(map_id)\n",
    "\n",
    "# Apply the mapping to the labels\n",
    "labels['Provider'] = labels['Provider'].map(map_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main strength of graph neural networks is that they can incorporate the node features into the learning pipeline. For this data set, however, no additional features are available for the health care providers. The implementation of the GNNs often requires some features two be present. To mitigate this, we mention two popular methods:\n",
    "* Give each node feature value 1, so the GNN can be trained;\n",
    "* Calculate some basic network features (degree, betweenness) to and add these as features to the nodes. \n",
    "\n",
    "In this notebook, we will proceed with the second option. The calculations will now be based on `networkx`, since the network has been constructed in that package. It is possible to do the calculations again in `igraph`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(graph.degree())\n",
    "betw_dict = nx.betweenness_centrality(graph)\n",
    "\n",
    "degree_dict = {map_id[k]:v for k,v in degree_dict.items()}\n",
    "betw_dict = {map_id[k]:v for k,v in betw_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_list = [degree_dict[i] for i in range(len(nodes))]\n",
    "betw_list = [betw_dict[i] for i in range(len(nodes))]\n",
    "\n",
    "nodes['degree'] = degree_list\n",
    "nodes['betweenness'] = betw_list\n",
    "\n",
    "nodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
